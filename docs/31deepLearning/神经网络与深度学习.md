## 《神经网络与深度学习》

+ https://nndl.github.io/nndl-book.pdf

### 全书

+ 以前并不知道**数学优化、信息论**

#### 整体框架

+ Part1（机器学习基础）1-3
+ Part2（基础模型）4-10
+ Part3（进阶模型）11-15
+ 附录（数学基础）

### 附录

#### A、线性代数

##### A.1、向量和向量空间

###### A.1.1、向量（Vector）

+ **标量（Scalar）**只有大小，没有方向

A.1.2、向量空间（）

A.1.3、范数（Norm）

+ *以前没听过*

##### A.2、矩阵

A.2.3、矩阵操作

+ *内积（标量积，点积），正交（内积为0），lionel，这部分没找到，不知道哪看的*

+ 外积
+ 秩（Rank）
  + **列秩**，线性无关的列向量数据（*这个怎么求，lionel*）
  + **行秩**，线性无关的行向量数据
  + 一个矩阵的行秩和列秩总是相等的

###### A2.6、矩阵分解

#### B、微积分

#### C、数学优化

#### D、概率论

##### D.1、样本空间

##### D.2、事件和概率

###### D.2.1、随机变量

+ 0

+ 2.1.1、离散随机变量
  + 随机变量取的值是**可列举的**
  + 引申到**概率分布**
  + 伯努利分布
  + 二项分布
  + **排列与组合**
    + 排列：N个元素排序，N的阶乘；N中取k个呢`(N)阶乘/(N-k)阶乘`，一般是`P`
    + 组合：取出指定元素，不考虑排序，`(N)阶乘/(k)阶乘 * (N-k)阶乘`，一般是`C`，也就是`排列数/k阶乘`
+ 2.1.2、连续随机变量
+ 2.1.3、累积分布函数

###### D.2.2、随机向量

+ 2.2.1、离散随机向量
+ 2.2.2、连续随机向量

##### D.3、随机过程（Stochastic Process）

#### E、信息论

##### E.1、熵（Entropy）

+ 物理学，表一个热力学系统的无序程度
+ 信息论中，**衡量一个随机事件的不确定性**

##### E.1.1、自信息和熵

### chap1、绪论

+ 深度学习
  + 1、是**机器学习**的问题，是其一个分支，*抽象成一类问题的解法*
  + 2、采用的模型比较复杂，**不清楚每个组件的贡献度**
    + 贡献度分配问题（Credit Assignment Problem），CAP，Minsky，1961
+ 目前较好解决贡献度分配的模型是**人工神经网络**（Artificial Neural Network），ANN，受人脑神经网络系统的工作方式启发而构造的，**并行的非线性信息处理系统**

#### 1.1、

#### 1.3、表示学习

#### 1.4、深度学习

+ **深度**，原始数据进行非线性特征转换的次数，用**有向图**表示的话，可以看成从输入到输出的**最长路径**的长度
+ 深度学习的主要目的，**从数据中自动学习到有效的特征表示**。

+ 复杂任务，需要切成多子模块（或多阶段），每个子模块分开学习，这会有2个问题
  + 1、每个模块需要单独优化，优化目标和总体目标并不能保持一致
  + 2、错误传播，即前一步的错误会对后续的模型造成很大的影响

##### 1.4.1、端到端学习（End-to-End Learning）

+ 也称为**端到端训练**，是指不进行分模块或分阶段训练，直接优化任务的总体目标
+ 也是为了解决**CAP问题**（贡献度分度）

#### 1.5、神经网络

##### 1.5.1、人脑神经网络

+ 神经元

### chap2、机器学习概述

+ 机器学习（Machine Learning）：**让计算机从数据中自动学习，得到某种知识（或规律）**
  + 作为一门学科，也表示一种解决问题的方法，**如何从样本中找到规律，并利用学习到的规律（模型）对未知或无法观测的数据进行预测**
+ 早期机器学习，主要称为**模式识别**（Pattern Recognition），偏向具体的应用任务
  + 手写体数字识别

#### 2.1、基本概念

+ 特征（Feature），也就是属性，大小、产地
+ 预测的**标签**（Label），可以是分数，比如80，90，也可以是好、坏
+ 标记好特征以及标签的苹果看成是一个**样本**（Sample）
+ 一组样本构成的集合称为**数据集**（Data Set），分两部分
  + 训练集（Training Set）
  + 测试集（Test Set）
+ D维向量（一个苹果的所有特征构成的向量），称为**特征向量**（Feature Vector），*这是个列向量，lionel*
  + 每一维表示一个特征
  + **标签用y表示**
+ *lionel，后面就没太明白了，公式*

#### 2.2、机器学习的三个基本要素

##### 2.2.1、模型

+ 机器学习，需要确定输入空间、输出空间，不同机器学习任务的主要区别在于**输出空间不同**
  + 二分类问题
  + C分类问题
  + 回归问题
+ *没太懂*

###### 2.2.1.1、线性模型

###### 2.2.1.1、非线性模型

+ *lionel，等价于神经网络*

##### 2.2.2、学习准则

+ 模型的好坏可以通过**期望风险**（Expected Risk）来衡量

###### 2.2.2.1、损失函数

+ **损失函数**，非负实数函数，量化模型预测和真实标签之间的差异
+ 常见的损失函数
  + 0-1
    + 最直观的损失函数是，模型在训练集上的错误率
  + 平方
    + 不适用于分类问题
  + 交叉熵
  + Hinge

###### 2.2.2.1、风险最小化准则

+ **过拟合**

##### 2.2.3、优化算法

###### 2.2.3.1、梯度下降法

###### 2.2.3.2、提前停止

###### 2.2.3.3、随机梯度下降法

###### 2.2.3.4、小批量梯度下降法

#### 2.3、机器学习的简单示例-线性回归

+ **线性回归**（Linear Regression），是一种对自变量和因变量之间关系进行建模的回归分析，自变量数量为1时称为**简单回归**，大于1时称为**多元回归**

##### 2.3.1、参数学习

###### 2.3.1.1、经验风险最小化

###### 2.3.1.2、结构风险最小化

###### 2.3.1.3、最大似然估计

###### 2.3.1.4、最大后验估计

#### 2.4、偏差-方差分解

#### 2.5、机器学习算法的类型

+ **按训练样本提供的信息以及反馈方式**，分为：
  + 监督
    + 回归
    + 分类
    + 结构化
  + 无监督
  + 强化

#### 2.6、数据的特征表示

+ 图像特征
+ 文本特征
+ 表示学习

##### 2.6.1、传统的特征学习

2.6.1.1、特征选择

2.6.1.2、特征抽取

##### 2.6.2、深度学习方法

#### 2.7、评价指标

+ 准确率
+ 错误率
+ 精确率和召回率
+ 宏平均和微平均
+ 交叉验证（Cross-Validation）

#### 2.8、理论和定理

##### 2.8.1、PAC学习理论

+ **计算学习理论**（Computational Learning Theory）
+ PAC学习分为两部分
  + 近似正确
  + 可能

##### 2.8.2、没有免费午餐定理

##### 2.8.3、奥卡姆剃刀原理

##### 2.8.4、丑小鸭原理

##### 2.8.5、归纳偏置

#### 2.9、总结和深入阅读

### chap3、线性模型

#### 3.1、线性判别函数和决策边界

##### 3.1.1、二分类

##### 3.1.2、多分类

3.2、Logistic回归

3.3、Softmax回归

3.4、感知机

3.5、支持向量机

##### 3.5.2、核函数

3.6、损失函数对比

3.7、总结和深入阅读

### chap5、卷积神经网络

#### 5.1、卷积（Convolution）

##### 5.1.1、卷积的定义

+ **卷积是一种运算**，分为一维和二维
  + *但具体怎么运算，要看一下，lionel*

+ *计算信号，延迟累积，lionel，ppt上看到的*

###### 5.1.1.1、一维卷积

+ **信息的衰减率**`W[k]`，k-1个时间步长后，信息为原来的`W[k]`倍。
  + 又称为**滤波器（Filter）或卷积核（Convolution Kernel）**

###### 5.1.1.2、二维卷积

### 最后

+ 李金洪《深度学习之TensorFlow：入门、原理与进阶实践》