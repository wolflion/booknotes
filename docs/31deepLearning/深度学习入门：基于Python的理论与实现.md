## 深度学习入门：基于Python的理论与实现

### chap1、Python入门

#### 1.5、NumPy

##### 1.5.3、

##### 1.5.5、广播

+ 形状不同的数组之间也可以进行运算。

##### 1.5.6、访问元素

#### 1.6、Matplotlib

##### 1.6.3、显示图像

### chap2、感知机（perceptron）

+ 1957年，Frank Rosenblatt
+ 严格来说，本章中的感知机，应该叫**人工神经元**

#### 2.1、感知机是什么

+ **感知机**接收多个输入信号，输出一个信号。
  + **信号**，只有2种取值（0和1）
  + 图中的圆圈叫**神经元**
  + 只有总和超过某个界限值时，才会输出1，也叫**神经元被激活**
  + 每个输入信号，对应一个**权重**（weight）

#### 2.2、简单逻辑电路

##### 2.2.1、与门（and gate）

+ `and`，and运算

##### 2.2.2、与非门和或门

+ `NAND gate`，Not AND
+ **真值表**

#### 2.3、感知机的实现

##### 2.3.1、简单的实现

##### 2.3.2、导入权重和偏置

##### 2.3.3、使用权重和偏置的实现

#### 2.4、感知机的局限性

##### 2.4.1、异或门

+ *为啥无法实现，自己要看一下，lionel*

##### 2.4.2、线性和非线性

+ **感知机的局限性就在于它只能表示由一条直线分割的空间。**
+ **非线性空间**：由曲线分割的空间
+ **线性空间**：由直线分割而成的空间

#### 2.5、多层感知机

+ **感知机的绝妙之处在于它可以“叠加层”**（通过叠加层来表示异或门）

##### 2.5.1、已有门电路的组合

+ 组合前面做好的与门、与非门、或门进行配置。

##### 2.5.2、异或门的实现

+ 图2-13，用感知机表示异或门

#### 2.6、从与非门到计算机

+ **多层感知机**可以实现比之前见到的电路更复杂的电路。

#### 2.7、小结

+ 感知机是神经网络的基础

### chap3、神经网络

+ 感知机中，**设定权重的工作**（是由人工进行的），即确定合适的、能符合预期的输入与输出的权重。
+ 神经网络（能解决这个问题），**可以自动季从数据中学习到合适的权重参数**。

#### 3.1、从感知机到神经网络

##### 3.1.1、神经网络的例子

+ 输入层、中间层、输出层
  + 中间层，有时也叫**隐藏层**，（隐藏层的神经元肉眼看不到）
+ 几层的描述
  + 有的是把**实质上拥有权重的层数**

##### 3.1.2、复习感知机

##### 3.1.3、激活函数登场

+ **激活函数**：`h(x)`将输入信号的总和转换为输出信号
+ $a=b+w_1x_1+w_2x_2$，然后$y=h(a)$，
+ **神经元**和**节点**等价
+ **激活函数**是连接感知机和神经网络的桥梁
  + 朴素感知机，指单层网络，指的是激活函数使用了**阶跃函数**的模型
  + 多层感知机，使用$sigmoid()$等平滑的激活函数的多层网络

#### 3.2、激活函数

+ **阶跃函数**：激活函数以阈值为界，一旦输入超过阈值，就切换输出。

##### 3.2.1、sigmoid函数

+ $h(x)=1/(1+e^{-x})$，e是纳皮尔常数2.7182...

##### 3.2.2、阶跃函数的实现

##### 3.2.3、阶跃函数的图形

3.2.4、sigmoid函数的实现

##### 3.2.5、sigmoid函数和阶跃函数的比较

+ 平滑性上
  + sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化
  + 阶跃函数以0为界，输出发生急剧性的变化

3.2.6、非线性函数

+ **神经网络的激活函数必须使用非线性函数**，因为用线性函数的话，加深神经网络的层数就没有意义

##### 3.2.7、ReLU函数（Rectified Linear Unit）

+ `h(x) {x>0时，h(x)=x;  x<=0时，h(x)=0;}`

#### 3.3、多维数组的运算

##### 3.3.3、神经网络的内积

#### 3.4、3层神经网络的实现

+ *lionel，重点是这个*

##### 3.4.1、符号确认

3.4.2、各层间信号传递的实现

3.4.3、代码实现小结

#### 3.5、输出层的设计

+ 机器学习问题大致分为**回归**和**分类**问题
  + 回归：根据某个输入预测一个（连续的）数值问题，*lionel，简单说就是输出一个值的问题*，恒等函数
  + 分类：数据属于哪一个类别的问题，softmax函数

##### 3.5.1 恒等函数和softmax函数

##### 3.5.2、实现softmax函数时的注意事项

##### 3.5.3、softmax函数的特征

##### 3.5.4、输出层的神经元数量

#### 3.6、手写数字识别

+ 使用学习到的参数，**先实现神经网络的“推理处理”**，这个推理处理也称为神经网络的**前向传播**（forward propagation）
+ 使用神经网解决问题，
  + **训练**，使用训练数据（学习数据）进行权重参数的学习
  + **推理**，使用学习到的参数，对输入数据进行分类

##### 3.6.1、MNIST数据集

##### 3.6.2、神经网络的推理处理

##### 3.6.3、批处理

3.7、小结

+ 本章介绍了神经网络的前向传播。

  + [日拱一卒：深度学习笔记5——神经网络的前向传播](https://www.bilibili.com/read/cv15303804/)，*放在下文了，这就比较好的解释了什么叫前向传播*

  > 在神经网络中，当我们已知每个节点对应的权重与偏置这些系数后，就可以通过输入X，不断向前推进，最终计算出输出Y，这就是神经网络的前向传播。

### chap4、神经网络的学习

+ 这里所说的“学习”是指从训练数据中自动获取最优权重参数的过程。

#### 4.1、从数据中学习

+ 所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。
+ *lionel，备注这个没看懂*

4.1.1 数据驱动

+ 考虑通过有效利用数据来解决这个问题（识别手写“5”）
  + 有3种办法【图4-2】
    + 1、人想到的算法
    + 2、人想到的特征量、机器学习
    + 3、神经网络（深度学习）
  + 方法1、先从图像中提取特征量，再用机器学习技术学习这些特征量的模式。
    + **特征量**：指可以从输入数据（输入图像）中准确地提取本质数据（重要的数据）的转换器。
    + 图像的特征量通常表示为向量的形式。
    + 在计算机视觉领域，常用的特征量包括SIFT、SURF和HOG等。
    + 使用这些特征量将图像数据转换为向量，然后对转换后的向量使用机器学习中的SVM、KNN等分类器进行学习。
  + 方法2、在神经网络中，连图像中包含的重要特征量也都是由机器来学习的

4.1.2 训练数据和测试数据

+ 为了正确评价模型的**泛化能力**，就必须划分训练数据和测试数据。
  + *lionel，泛化能力是啥？*
+ 训练数据也可以称为**监督数据**。
+ 只对某个数据集过度拟合的状态称为过拟合（over fitting），*可以顺利地处理某个数据集，但无法处理其他数据集的情况*

#### 4.2、损失函数

+ 神经网络以某个指标为线索寻找**最优权重参数**。
  + 这个指标称为损失函数（loss function）。

##### 4.2.1、均方误差

4.3、数值微分

4.4、梯度

#### 4.5、学习算法的实现

+ 神经网络学习分为4步：
  + 1、mini-batch
    + 从训练数据中随机选出一部分数据，这部分数据称为mini-batch。
    + 我们的目标是减小mini-batch的损失函数的值。
  + 2、计算梯度
    + 为了减小mini-batch的损失函数的值，需要求出各个权重参数的梯度。
    + **梯度**表示损失函数的值减小最多的方向。
  + 3、更新参数
    + 将权重参数沿梯度方向进行微小更新。
  + 4、重复1、2、3

##### 4.5.1 2层神经网络的类

4.6、小结