### chap1、基于可扩展哈希的受控副本分布策略CRUSH

#### 0、

+ CRUSH（Controlled Replication Under Scalable Hashing），**基于哈希的数据分布算法**

#### 1.1、straw及straw2算法简介

+ straw算法执行结果取决于三个因素：**固定输入、元素编号和元素权重**。

#### 1.2、CRUSH算法详解

##### 1.2.1、集群的层次化描述-Cluster Map

##### 1.2.2、数据分布策略--Placement Rule

  + 每条placment rule可以包含多个操作，这些操作共有3种类型
   + （1）take
   + （2）select
   + （3）emit

#### 1.3、调制CRUSH

0、

+ **提升CRUSH的计算效率**

##### 1.3.1、编辑CRUSH Map

+ （1）获取CRUSH map
+ （2）反编译CRUSH map
+ （3）编辑CRUSH map
+ （4）编译CRUSH map
+ （5）模拟测试
+ （6）注入集群

##### 1.3.2、定制CRUSH规则

##### 1.3.3、数据重平衡

#### 1.4、总结与展望

+ list和tree算法被先后废弃，原创的straw算法被完全重写

### chap2、新型对象存储引擎BlueStore

https://github.com/ceph/ceph/tree/main/src/os/bluestore
0、

#### 2.1、设计理念与指导原则

+ 写中间设备的过渡过程称为**写日志**，中间设备也叫**日志设备**，一般可由NVRAM或者SSD等高速存储设备充当。
+ 几个术语
  + block-size（块大小）
  + RMW（Read Modify Write）
  + COW（Copy-On-Write）
+ BlueStore针对写操作综合运用了RMW和COW策略-任何一个写请求，根据磁盘块大小，将其切分为三个部分：
  + 首尾非块大小对齐部分和中间块大小对齐部分
  + 然后针对中间块对齐部分采用COW策略
  + 首尾非块对齐部分采用RMW策略

#### 2.2、磁盘数据结构

0、

+ **BuleStore中习惯上磁盘格式以"`_t`"结尾并且全部小写，而内存格式不以"`\t`"结尾并且只有首字母大写。

##### 2.2.1、PG

+ Ceph并不是将任何上层数据一步到位映射到磁盘（OSD），而是引入了一个中间结构，称为**PG，实现两级映射**
  + 第一级映射是静态的
  + 第二级映射实现PG到OSD的映射，**仍然采用伪随机哈希函数**
+ PG对应的磁盘结构称为`bluestore_cnode_t`

##### 2.2.2、对象

+ extent是对象内的基本数据管理单元，下面分别予以阐述：
  + 数据校验
  + 数据压缩
  + 数据共享

#### 2.3、缓存管理

##### 2.3.1、常见的缓存淘汰算法

##### 2.3.2、BlueStore中的缓存管理

+ 2类型的缓存算法

  + LRU
  + 2Q：**类似于简化版本ARC**，区别在于只使用一个影子队列

#### 2.4、磁盘空间管理

##### 2.4.1、常见磁盘空间管理模式

##### 2.4.2、BitmapFreelistManager

##### 2.4.3、BitmapAllocator

#### 2.5、BlueFs

##### 2.5.1、RocksDB与BuleFs

##### 2.5.2、磁盘数据结构

##### 2.5.3、块设备

#### 2.6、实现原理

##### 2.6.1、mkfs

+ **mkfs主要固化一些用户指定的配置项到磁盘，这样后续BlueStore上电时，这些配置项将直接从磁盘读取**。

##### 2.6.2、mount

+ OSD进程上电时，BlueStore通过mount操作完成正常上电前的检查和准备工作。**mount操作包含以下几个步骤**：
  + 1、校验ObjectStore类型
  + 2、fsck或者deep-fsck
  + 3、加载并锁定fsid
  + 4、加载主块设备
  + 5、加载数据库，读取元数据
  + 6、加载Collection

##### 2.6.3、read

##### 2.6.4、write

#### 2.7、使用指南

##### 2.7.1、部署BlueStore

##### 2.7.2、配置参数

#### 2.8、总结与展望

### chap3、纠删码原理与overwrites支持

#### 0、

+ 纠删码（Erasure Coding）
+ 纠删码最简单的实现方式是完全复制，也称为镜像，**将数据不做任何处理同时写入多个不同的存储介质**。
+ overwrites技术（覆盖写）

#### 3.1、RAID技术概述

+ 数据存储在单个磁盘上，存在一些固有缺陷
  + 访问速度慢
  + 容量小
  + 安全性差
+ 主流的RAID技术
  + RAID0
  + RAID1
  + RAID5
  + RAID6

#### 3.2、RS-RAID和Jerasure

  0、

##### 3.2.1、计算校验和

##### 3.2.2、数据恢复

##### 3.2.3、算术运算

##### 3.2.4、缺陷与改进

##### 3.2.5、Jerasure

#### 3.3、纠删码在Ceph中的应用

##### 3.3.1、术语

##### 3.3.2、概述

##### 3.3.3、新写

##### 3.3.4、读

##### 3.3.5、覆盖写

##### 3.3.6、日志

##### 3.3.7、Scrub

#### 3.4、总结与展望

### chap5、存储服务质量QoS

#### 0、

+ dmClock用于分布式系统的I/O调度算法，（Handling Throughput Variablity for Hypervisor IO Scheduling）

#### 5.1、研究现状

+ BTRFS
+ Q-EBOFS：**没有被Ceph采用**
  + 在BTRFS原有磁盘调度队列中，引入了一层客户端级别的新队列，采用基于权重的轮询机制，将不同客户端队列中的请求分发至磁盘调度队列，从而在单个OSD层面实现对不同客户端按其权重分配I/O资源的目标。

#### 5.2、dmClock算法原理

##### 5.2.1、mClock

+ 基于时间标签的I/O调度算法。
+ mClock基本原理主要包含三个方面：
  + 为每个客户端设置一套QoS模板参数，包括预留（r）、权重（w）和上限（l）
  + 服务端分两个阶段来处理I/O请求：
    + 一是基于强制的Constraint-based阶段
    + 二是基于权重的Weight-based阶段
  + 服务端优先工作于Constraint-based阶段

##### 5.2.2、dmClock

+ 是mClock算法的分布式版本，**两者的基本原理相同**
+ dmClock和mClock的差别：
  + 分布式系统具有多个服务器
  + 客户端记录每个服务器完成的请求个数
  + 服务端计算请求的时间标签时，

#### 5.3、QoS的设计与实现

##### 0、

+ **通常将QoS的实现置于每个OSD中**，每个OSD通过一个称为**ShardedOpWQ的工作队列**

##### 5.3.1、优先级队列（prio）

+ 入队
+ 出队

##### 5.3.2、权重的优先级队列（wpq）

+ 基于权重的优先级队列与PrioritizedQueue的队列结构类似，同样分为优先级prio、客户端client以及真实的请求list三个级别。
+ 请求的出队机制：
  + 采用权重概率的方式确定prio级别
  + 被选中的prio队列并不定能出队请求
  + client级别和真实请求的选择采用与PrioritizedQueue相同的方式，分别为**轮询和FIFO策略**

##### 5.3.3、dmClock队列

+ dmClock是一个两级映射队列，第一级为客户端的client队列，第二级为真实的请求队列，每个请求包含三个时间标签`<RWL>`
+ **dmClock采用完全二叉树来组织管理**

##### 5.3.4、Client的设计

#### 5.4、总结与展望

+ 可能的改进方向包括以下几个方面：
  + 1、合理模板参数的设置
  + 2、I/O带宽的限制
  + 3、突发I/O的处理

### chap6、分布式块存储RBD

#### 0、

+ RBD（RADOS Block Device）

#### 6.1、RBD架构

+ 上层应用访问RBD块设备有两种途径：
  + librbd：基于librados的用户态接口库
  + krbd：集成在GNU/Linux内核的一个内核模块，通过用户态的rbd命令行工具，可以将RBD块设备映射为本地的一个块设备文件

#### 6.2、存储组织

##### 0、

+ RBD块设备在Ceph中被称为image。**image由元数数据和数据两部分组成**
+ v1格式的image，v2格式的image

##### 6.2.1、元数据

##### 6.2.2、数据

#### 6.3、功能特性

##### 6.3.1、快照

##### 6.3.2、克隆

#### 6.4、总结

### chap7、对象存储网关RGW

#### 7.1、总体架构

#### 7.2、数据组织和存储

##### 7.2.1、用户

##### 7.2.2、存储桶

##### 7.2.3、对象

##### 7.2.4、数据存储位置

#### 7.3、功能实现

##### 7.3.1、功能特性

+ 对象存储最基本的功能包括**用户、存储桶、对象的增删改查**
+ RGW支持的S3 API
+ RGW支持的Swift API
+ RGW常用的Admin API
  + Get Usage

##### 7.3.2、I/O路径

+ 0、
  + op thread
+ 1、用户认证
+ 2、用户/存储桶/对象访问权限控制
+ 3、bucket/用户配额

##### 7.3.3、存储桶创建

##### 7.3.4、对象上传

##### 7.3.5、对象下载

#### 7.4、总结与展望

### chap8、分布式文件系统CephFS

0、

+ **CephFS基于MDS（MetaData Server）对元数据进行管理**。

#### 8.1、文件系统基础知识

##### 8.1.1、文件系统

+ VFS

##### 8.1.2、文件系统中的元数据

+ 定义4种基本数据类型：
  + SuperBlock
  + Inode
  + Dentry
  + File
+ **SuperBlock和File用于面向前端（客户端）提供服务，而Inode和Dentry则需要后端（服务端）文件系统提供服务功能**，重点介绍下后2者

  + Inode
+ Dentry

##### 8.1.3、硬链接和软链接

+ **硬链接**：指多个文件名指向同一个Inode号，**相同的存储内容可以合适不同的文件名**，目录不能用来创建硬链接
+ **软链接**：创建一个新的Inode，**Inode存储的内容是另外一个文件路径名的指向**

##### 8.1.4、日志

+ 日志文件系统，常见3种设计模式
  + writeback模式
  + ordered模式
  + data模式（writeahead）
+ 日志提交有两种常见的策略：
  + 超时提交
  + 满时提交

#### 8.2、分布式文件系统CephFS

##### 8.2.1、CephFS设计框架和背景

+ 为了实现文件系统数据（包含元数据和用户数据）负载均衡，业界有如下几种分区方法：
  + （1）静态子树分区
  + （2）Hash计算分区法
  + （3）动态子树分区
+ CephFS一共存在如下三种形式的客户端（接口）：
  + （1）CephFS Kernel Object
  + （2）CephFS FUSE
  + （3）User Space Client

##### 8.2.2、MDS的作用

+ **MDS用于保存CephFS的元数据信息，它是运行在Ceph服务侧的守护进程**，使用动态申请缓存空间来存储元数据信息，其记录的元数据除了文件在磁盘中的位置，还包括文件名、文件属性、归属目录、子树分割以及诸如快照、配额在内的一些高级特性。
+ MDS在设计和实现上具有以下特点：
  + 首先，Ceph的基本存储单元是对象
  + 其次，每个MDS独立更新自己的日志
  + 最后，动态子树分区实现了文件系统的动态负载均衡

#### 8.3、MDS设计原理与实现

##### 8.3.1、MDS元数据存储

	  0、

+ **MDS的元数据和业务数据都存储在RADOS对象（统称为Object）中**。
  1、元数据与Object的关系
  2、嵌入式Inode和Primary Dentry
  3、Remote Dentry和Anchor表
  4、日志
+ MDS的日志系统使用如下混合模式

##### 8.3.2、MDS负载均衡实现

  1、元数据负载实现背景
  2、目录分片
  3、子树分区
  4、元数据复制
  5、锁机制
  6、负载均衡
  7、子树迁移
  8、流量控制

##### 8.3.3、MDS故障恢复

  1、日志在故障恢复中的作用
  2、故障检测
  3、MDS恢复

+ （1）Replay
+ （2）Resolve
+ （3）Reconnect
+ （4）Rejoin

#### 8.4、总结与展望