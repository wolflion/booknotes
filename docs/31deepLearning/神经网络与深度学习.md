## 《神经网络与深度学习》

+ https://nndl.github.io/nndl-book.pdf

### 全书

+ 以前并不知道**数学优化、信息论**

#### 整体框架

+ Part1（机器学习基础）1-3
+ Part2（基础模型）4-10
+ Part3（进阶模型）11-15
+ 附录（数学基础）

### 附录

#### A、线性代数

##### A.1、向量和向量空间

###### A.1.1、向量（Vector）

+ **标量（Scalar）**只有大小，没有方向

A.1.2、向量空间（）

A.1.3、范数（Norm）

+ *以前没听过*

##### A.2、矩阵

A.2.3、矩阵操作

+ *内积（标量积，点积），正交（内积为0），lionel，这部分没找到，不知道哪看的*

+ 外积
+ 秩（Rank）
  + **列秩**，线性无关的列向量数据（*这个怎么求，lionel*）
  + **行秩**，线性无关的行向量数据
  + 一个矩阵的行秩和列秩总是相等的

###### A2.6、矩阵分解

#### B、微积分

#### C、数学优化

#### D、概率论

##### D.1、样本空间

##### D.2、事件和概率

###### D.2.1、随机变量

+ 0

+ 2.1.1、离散随机变量
  + 随机变量取的值是**可列举的**
  + 引申到**概率分布**
  + 伯努利分布
  + 二项分布
  + **排列与组合**
    + 排列：N个元素排序，N的阶乘；N中取k个呢`(N)阶乘/(N-k)阶乘`，一般是`P`
    + 组合：取出指定元素，不考虑排序，`(N)阶乘/(k)阶乘 * (N-k)阶乘`，一般是`C`，也就是`排列数/k阶乘`
+ 2.1.2、连续随机变量
+ 2.1.3、累积分布函数

###### D.2.2、随机向量

+ 2.2.1、离散随机向量
+ 2.2.2、连续随机向量

##### D.3、随机过程（Stochastic Process）

#### E、信息论

##### E.1、熵（Entropy）

+ 物理学，表一个热力学系统的无序程度
+ 信息论中，**衡量一个随机事件的不确定性**

##### E.1.1、自信息和熵

### chap1、绪论

+ 深度学习
  + 1、是**机器学习**的问题，是其一个分支，*抽象成一类问题的解法*
  + 2、采用的模型比较复杂，**不清楚每个组件的贡献度**
    + 贡献度分配问题（Credit Assignment Problem），CAP，Minsky，1961
+ 目前较好解决贡献度分配的模型是**人工神经网络**（Artificial Neural Network），ANN，受人脑神经网络系统的工作方式启发而构造的，**并行的非线性信息处理系统**

#### 1.1、

#### 1.3、表示学习

#### 1.4、深度学习

+ **深度**，原始数据进行非线性特征转换的次数，用**有向图**表示的话，可以看成从输入到输出的**最长路径**的长度
+ 深度学习的主要目的，**从数据中自动学习到有效的特征表示**。

+ 复杂任务，需要切成多子模块（或多阶段），每个子模块分开学习，这会有2个问题
  + 1、每个模块需要单独优化，优化目标和总体目标并不能保持一致
  + 2、错误传播，即前一步的错误会对后续的模型造成很大的影响

##### 1.4.1、端到端学习（End-to-End Learning）

+ 也称为**端到端训练**，是指不进行分模块或分阶段训练，直接优化任务的总体目标
+ 也是为了解决**CAP问题**（贡献度分度）

#### 1.5、神经网络

##### 1.5.1、人脑神经网络

+ 神经元

### chap5、卷积神经网络

#### 5.1、卷积（Convolution）

##### 5.1.1、卷积的定义

+ **卷积是一种运算**，分为一维和二维
  + *但具体怎么运算，要看一下，lionel*

+ *计算信号，延迟累积，lionel，ppt上看到的*

###### 5.1.1.1、一维卷积

+ **信息的衰减率**`W[k]`，k-1个时间步长后，信息为原来的`W[k]`倍。
  + 又称为**滤波器（Filter）或卷积核（Convolution Kernel）**

###### 5.1.1.2、二维卷积

### 最后

+ 李金洪《深度学习之TensorFlow：入门、原理与进阶实践》