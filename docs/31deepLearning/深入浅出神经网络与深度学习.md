## 《深入

### chap1、使用神经网络识别手写数字

+ **人类的视觉系统**
  + 初级视觉皮层，V1，1.4亿个神经元
+ 神经网络**使用样本**来自动推断识别手写数字的规则

#### 1.1、感知机

+ 自己问题：
  + 怎么去记感知机？

#### 1.2、sigmod神经元

+ 自己问题：
  + sigmod函数

#### 1.3、神经网络的架构

+ **隐藏层**（非输入和输出层）
+ 多层感知机（MLP），multilayer perceptron，**多层神经网络是由sigmod神经元构成，而非感知机**，只是习惯了叫这个名
+ **前馈神经网络**，以上一层输出作为下一层的输入，**没有回路**
+ **循环神经网络**，包含反馈回路  13(32/249)
  + 设计思想：**部分神经元在休眠前会保持激活状态，这种激活状态可以刺激其他神经元，将其激活并保持一段时间，这样会导致更多神经元被激活**。

#### 1.4、一个简单的神经网络：分类手写数字

+ 第一步，**如何分割图片**

#### 1.5、利用梯度下降算法进行学习

+ *为什么是梯度下降算法？*

#### 1.6、实现分类数字的神经网络

+ 包含这种多层结构（两层或更多隐藏层）的神经网络称为**深度神经网络**。

#### 1.7、迈向深度学习

### chap2、反向传播算法工作原理

+ 第1章介绍了神经网络如何使用梯度下降算法来学习权重和偏置，但其中存在一个问题：没有讨论如何计算代价函数的梯度。
+ Learning representations by back-propagating errors.【论文】1986
+ 反向传播的核心是**对代价函数 C 关于任何权重 w （或者偏置 b ）的偏导数**的表达式。

#### 2.1、热身：使用矩阵快速计算输出

2.2 关于代价函数的两个假设

### chap3、改进神经网络的学习方法

+ 4 种正则化方法（L1正则化、L2正则化、Dropout和人为扩展训练数据）

#### 3.1、交叉熵代价函数

+ *引入问题*

##### 3.1.1、引入交叉熵代价函数

+ **交叉熵**

##### 3.1.2、使用交叉熵来对 MNIST 数字进行分类

##### 3.1.3、交叉熵的含义与起源

##### 3.1.4、softmax

#### 3.2、过拟合和正则化

##### 3.2.1、正则化

+ *它通过在损失函数中添加一个惩罚项来实现，该惩罚项与模型的复杂度成正比。*(AI说的-lionel)

##### 3.2.2、为何正则化有助于减轻过拟合

##### 3.2.3、其他正则化技术

#### 3.3、权重初始化

#### 3.4、复探手写识别问题：代码

#### 3.5、如何选择神经网络的超参数

#### 3.6、其他技术

### chap4、神经网络可以计算任何函数的可视化证明  134（153/249）